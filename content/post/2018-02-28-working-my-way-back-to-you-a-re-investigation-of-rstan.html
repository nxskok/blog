---
title: Working my way back to you, a re-investigation of rstan
author: Ken
date: '2018-02-28'
slug: working-my-way-back-to-you-a-re-investigation-of-rstan
categories:
  - R
  - statistics
  - Bayesian
tags:
  - '#rstats'
  - '#rstan'
---



<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>I learned Stan a while back, when I was fitting some Bayesian models. I wanted to fix up one of them, and I realized that I had forgotten most of what I knew about Stan, so I had to go back and learn it again.</p>
<p>A Bayesian model has two parts: a prior distribution, which summarizes your belief about the parameters you are trying to estimate before you look at any data, and a model that asserts the data-generating mechanism conditional on the parameter value(s). These are combined into a posterior distribution which <em>is</em> your inference about your parameter(s) based on your prior beliefs and on the data. I learned my Bayesian statistics from <a href="http://www.stat.sfu.ca/people/faculty/history/Villegas.html">this guy</a> and I can still hear him saying, with Uruguayan accent, “posterior is proportional to likelihhhhhood times pri-or”. What you do is to summarize your posterior distribution somehow, its mean or mode or a highest posterior density interval (or region), which plays the same role as a confidence interval (or region). You can also generate simulated data from the posterior distribution, called a <strong>predictive distribution</strong>. An important part of model checking is to compare the distribution of your data with the predictive distribution. If they match reasonably well, you can have confidence in your model. This is something <a href="http://www.stat.columbia.edu/~gelman/book/">Gelman et al</a> encourage doing.</p>
<p>Stan provides a mechanism for randomly sampling from a posterior distribution, and enables you to do Bayesian inference for parameters (and such things as posterior predictive distributions) without having to worry about whether the model you are trying to fit is analytically tractable.</p>
<p><label for="tufte-mn-" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-" class="margin-toggle"><span class="marginnote">The old-fashioned way of doing Bayesian inference required you to derive the posterior distribution using algebra; if you were lucky, it would be of a form you could recognize, but most of the time you would not be lucky. There was a lot of use at that time of so-called conjugate priors that were not necessarily reasonable, something I imagine that has now been consigned to the dustbin of history.</span></p>
<p>The idea of sampling from the posterior distribution has revolutionized Bayesian statistics; basically, if you can write down a model, you can sample from the posterior distribution using Metropolis-Hastings or one of the modern variants that Stan uses, and then use the randomly generated posterior distribution as if it were the real thing.</p>
<p>As I was thinking about this post and re-learning Stan, I was reminded of a song from my youth called <a href="https://www.youtube.com/watch?v=jHEiZy53r6k">Working My Way Back To You Babe</a>. I grew up in England, so this group was known to me as the Detroit Spinners, not to be confused with a British folk band of the time called <a href="https://en.wikipedia.org/wiki/The_Supinners_(UK_band)">The Spinners</a>.</p>
</div>
<div id="starting-point-estimating-a-poisson-mean" class="section level2">
<h2>Starting point: estimating a Poisson mean</h2>
<p>The <a href="https://www.umass.edu/wsp/resources/poisson/">Poisson distribution</a> is a model for counted data, such as goals in soccer or hockey or other relatively rare events like the number of flaws in a piece of cloth. The value it has for us is that it only has one parameter to estimate, called <span class="math inline">\(\lambda\)</span>, which is both the mean and variance of the distribution.</p>
<p>Let’s make things easy for ourselves by generating some data that actually <em>is</em> from a Poisson distribution with a mean that we know, and see whether the estimation process recovers the true mean. (This is a good way of developing Stan code: try it out first on data known to be from the model you are trying to fit, and when that seems to be behaving itself, run the same code on your actual data.)</p>
<p>Stan runs either independently (via the command-line version of Stan called <code>cmdstan</code>) or connected with other languages such as R, Python, <a href="http://mc-stan.org/users/interfaces/index.html">and others</a>. We will use the R interface, which works via the package <code>rstan</code>. I probably need the <code>tidyverse</code> as well, and I will be reading an Excel sheet in later:</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ──────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 2.2.1.9000     ✔ purrr   0.2.4     
## ✔ tibble  1.4.2          ✔ dplyr   0.7.4     
## ✔ tidyr   0.8.0          ✔ stringr 1.3.0     
## ✔ readr   1.1.1          ✔ forcats 0.3.0</code></pre>
<pre><code>## ── Conflicts ─────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(readxl)
library(rstan)</code></pre>
<pre><code>## Loading required package: StanHeaders</code></pre>
<pre><code>## rstan (Version 2.17.2, GitRev: 2e1f913d3ca3)</code></pre>
<pre><code>## For execution on a local, multicore CPU with excess RAM we recommend calling
## options(mc.cores = parallel::detectCores()).
## To avoid recompilation of unchanged Stan programs, we recommend calling
## rstan_options(auto_write = TRUE)</code></pre>
<pre><code>## 
## Attaching package: &#39;rstan&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:tidyr&#39;:
## 
##     extract</code></pre>
<p>Let’s generate some random Poisson data with mean 3. I set the random number seed first so that this is reproducible:</p>
<pre class="r"><code>set.seed(457299)
x=rpois(50,3)
x</code></pre>
<pre><code>##  [1]  6  1  2  3  2  4  2  2  3  5  4  2  1  3  4  4  3  2  5  3  0  2  1
## [24]  3  5  3  4  4  2  4  3  1  6  2  3  5  3 11  0  5  5  6  4  3  5  2
## [47]  2  2  3  1</code></pre>
<p>50 random integers. The highest observed value is 11.</p>
<p>The non-Bayesian “frequentist” inference here would estimate <span class="math inline">\(\lambda\)</span> using the sample mean:</p>
<pre class="r"><code>mean(x)</code></pre>
<pre><code>## [1] 3.22</code></pre>
<p>and obtain an interval estimate by leaning on the central limit theorem (with a sample size of 50, this seems reasonable):</p>
<pre class="r"><code>t.test(x,mu=3)</code></pre>
<pre><code>## 
##  One Sample t-test
## 
## data:  x
## t = 0.82415, df = 49, p-value = 0.4138
## alternative hypothesis: true mean is not equal to 3
## 95 percent confidence interval:
##  2.683563 3.756437
## sample estimates:
## mean of x 
##      3.22</code></pre>
<p>The sample mean is somewhere near the true value of 3, and the 95% confidence interval does indeed contain 3.</p>
<p>To make this fly in Stan, we have to write a program in the Stan programming language (that looks a lot like R, in fact). This program is then turned into C++ code and then compiled, and then we can use it to sample from the posterior distribution. I build up the program step by step.</p>
<p>The place to start thinking is about the model. The thing we observed was called <code>x</code>, and this had a Poisson distribution with a mean <code>lambda</code> that we are trying to estimate. This goes in a section of the program called <code>model</code> (there will be others later):</p>
<pre><code>model {
// likelihood
x ~ poisson(lambda);
}</code></pre>
<p>The line starting with the slashes is a comment; the squiggle is read “is distributed as” in the familiar way. The <a href="https://github.com/stan-dev/stan/releases/download/v2.17.0/stan-reference-2.17.0.pdf">Stan manual</a>, chapters 49-64, details the distributions that are available and how to code them in Stan.</p>
<p>In Stan terms, everything in the model is either a parameter or data. In our case, <code>lambda</code> is a parameter and <code>x</code> is data. All our parameters need to have prior distributions. What I’m willing to assume about <code>lambda</code> without looking at my data is that it’s positive and not too big. I picked a chi-squared distribution with 5 df (which has mean 5) for the prior distribution. I could have gained more generality by using a gamma distribution or some other distribution on the positive real numbers. I add the prior to the <code>model</code> section thus:</p>
<pre><code>model {
// prior
lambda ~ chi_square(5);
// likelihood
x ~ poisson(lambda);
}</code></pre>
<p>The prior could have parameters of its own, so-called “hyperparameters”. For example, below I’ll let the prior mean be specified as part of the data.</p>
<p>Now that we know what the model, parameters and data are, we have to complete <code>data</code> and <code>parameters</code> sections that specify what kind of thing <code>x</code> and <code>lambda</code> actually are. Stan has two built-in data types, <code>real</code> (real numbers) and <code>int</code> (integers). You can declare arrays or matrices by adding a dimension to the declaration. In our case <code>lambda</code> is just a real number, but our data <code>x</code> is a vector with 50 entries, so we could proceed by adding these <code>data</code> and <code>parameters</code> sections:</p>
<pre><code>data {
int x[50];
}

parameters {
real&lt;lower=0&gt; lambda;
}

model {
// prior
lambda ~ chi_square(5);
// likelihood
x ~ poisson(lambda);
}</code></pre>
<p>but it seems unnecessarily restrictive to hard-code the sample size and prior mean, since I might want to run this again with different data, so we’ll declare <code>x</code> of length <code>n</code> and have <code>n</code> be part of our <code>data</code> section as well, in addition to adding the prior mean there:</p>
<pre><code>data {
int&lt;lower=1&gt; n;   
int x[n];
real prior_mean;
}

parameters {
real&lt;lower=0&gt; lambda;
}

model {
// prior
lambda ~ chi_square(prior_mean);
// likelihood
x ~ poisson(lambda);
}</code></pre>
<p>Anything can have limits, expressed in angle brackets after the type. Here <code>n</code> is an integer that is at least 1, and <code>lambda</code> is zero or bigger.</p>
<p>That is a functional Stan program, but I want to do a “posterior predictive” check of my model: that is, I want to generate a predictive distribution and compare it with my observed data. This is done by adding a <code>generated quantities</code> section to the end of my program, as shown below. To compile it, you can either store your program in a file, or, as here, as a text string:</p>
<pre class="r"><code>model=&#39;
data {
int&lt;lower=1&gt; n;   
int x[n];
real prior_mean;
}

parameters {
real&lt;lower=0&gt; lambda;
}

model {
// prior
lambda ~ chi_square(prior_mean);
// likelihood
x ~ poisson(lambda);
}

generated quantities {
  int x_new;
  x_new=poisson_rng(lambda);
}
&#39;</code></pre>
<p>In <code>generated quantities</code>, this says that <code>x_new</code> is obtained by sampling a random Poisson for each value of <code>lambda</code> in the posterior distribution (<code>x_new</code> is declared first; since it only appears in <code>generated quantities</code>, it can be declared there, it being neither a parameter nor data.) Thus there will be as many <code>x_new</code> values as <code>lambda</code> values.</p>
<p>I like to compile my model first, rather than compiling it each time I run my model, because that is the slowest part of the enterprise. This is how it goes:</p>
<pre class="r"><code>x_compiled=stan_model(model_code = model)</code></pre>
<pre><code>## In file included from /home/ken/R/x86_64-pc-linux-gnu-library/3.4/BH/include/boost/config.hpp:39:0,
##                  from /home/ken/R/x86_64-pc-linux-gnu-library/3.4/BH/include/boost/math/tools/config.hpp:13,
##                  from /home/ken/R/x86_64-pc-linux-gnu-library/3.4/StanHeaders/include/stan/math/rev/core/var.hpp:7,
##                  from /home/ken/R/x86_64-pc-linux-gnu-library/3.4/StanHeaders/include/stan/math/rev/core/gevv_vvv_vari.hpp:5,
##                  from /home/ken/R/x86_64-pc-linux-gnu-library/3.4/StanHeaders/include/stan/math/rev/core.hpp:12,
##                  from /home/ken/R/x86_64-pc-linux-gnu-library/3.4/StanHeaders/include/stan/math/rev/mat.hpp:4,
##                  from /home/ken/R/x86_64-pc-linux-gnu-library/3.4/StanHeaders/include/stan/math.hpp:4,
##                  from /home/ken/R/x86_64-pc-linux-gnu-library/3.4/StanHeaders/include/src/stan/model/model_header.hpp:4,
##                  from file795b75f52af8.cpp:8:
## /home/ken/R/x86_64-pc-linux-gnu-library/3.4/BH/include/boost/config/compiler/gcc.hpp:186:0: warning: &quot;BOOST_NO_CXX11_RVALUE_REFERENCES&quot; redefined
##  #  define BOOST_NO_CXX11_RVALUE_REFERENCES
##  ^
## &lt;command-line&gt;:0:0: note: this is the location of the previous definition</code></pre>
<p>The last thing we need is to set up the data. Looking at the <code>data</code> section, we see that we need to supply an <code>n</code>, a vector called <code>x</code> of length <code>n</code>, and a prior mean, which we do in a list:</p>
<pre class="r"><code>my_data=list(n=50,x=x,prior_mean=5)</code></pre>
<p>and then we can use this to sample from our posterior distribution, thus:</p>
<pre class="r"><code>pp1=sampling(x_compiled,data=my_data)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;97d657b45f98f78c52f7aedd24c13437&#39; NOW (CHAIN 1).
## 
## Gradient evaluation took 1.2e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  200 / 2000 [ 10%]  (Warmup)
## Iteration:  400 / 2000 [ 20%]  (Warmup)
## Iteration:  600 / 2000 [ 30%]  (Warmup)
## Iteration:  800 / 2000 [ 40%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.019579 seconds (Warm-up)
##                0.012955 seconds (Sampling)
##                0.032534 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;97d657b45f98f78c52f7aedd24c13437&#39; NOW (CHAIN 2).
## 
## Gradient evaluation took 5e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  200 / 2000 [ 10%]  (Warmup)
## Iteration:  400 / 2000 [ 20%]  (Warmup)
## Iteration:  600 / 2000 [ 30%]  (Warmup)
## Iteration:  800 / 2000 [ 40%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.012881 seconds (Warm-up)
##                0.013238 seconds (Sampling)
##                0.026119 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;97d657b45f98f78c52f7aedd24c13437&#39; NOW (CHAIN 3).
## 
## Gradient evaluation took 4e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  200 / 2000 [ 10%]  (Warmup)
## Iteration:  400 / 2000 [ 20%]  (Warmup)
## Iteration:  600 / 2000 [ 30%]  (Warmup)
## Iteration:  800 / 2000 [ 40%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.012738 seconds (Warm-up)
##                0.012887 seconds (Sampling)
##                0.025625 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;97d657b45f98f78c52f7aedd24c13437&#39; NOW (CHAIN 4).
## 
## Gradient evaluation took 4e-06 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  200 / 2000 [ 10%]  (Warmup)
## Iteration:  400 / 2000 [ 20%]  (Warmup)
## Iteration:  600 / 2000 [ 30%]  (Warmup)
## Iteration:  800 / 2000 [ 40%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.013028 seconds (Warm-up)
##                0.013759 seconds (Sampling)
##                0.026787 seconds (Total)</code></pre>
<p>The sampling process is tweakable. I used the default four chains of length 2000, with the first half of each chain being warmup and the second half being actual sampling. (“Warmup” is not the same as “burn-in” that you might be familiar with from Metropolis-Hastings.)</p>
<p>Looking at <code>pp1</code> gives us some information about the simulation:</p>
<pre class="r"><code>pp1</code></pre>
<pre><code>## Inference for Stan model: 97d657b45f98f78c52f7aedd24c13437.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##         mean se_mean   sd  2.5%   25%   50%   75% 97.5% n_eff Rhat
## lambda  3.24    0.01 0.25  2.75  3.06  3.23  3.41  3.73  1430    1
## x_new   3.20    0.03 1.81  0.00  2.00  3.00  4.00  7.00  3889    1
## lp__   28.08    0.02 0.70 26.00 27.91 28.35 28.53 28.59  1828    1
## 
## Samples were drawn using NUTS(diag_e) at Wed Mar 21 12:19:25 2018.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>The first line tells us about the posterior distribution of our parameter <code>lambda</code>, with a 95% posterior interval of 2.75 to 3.73 (safely containing the true value of 3). The second line <code>x_new</code> is our predictive distribution, that we will shortly compare for shape with our data. (The last line is the log-posterior density function.)</p>
<p>We should convince ourselves that the sampling has worked, which is done with this plot:</p>
<pre class="r"><code>traceplot(pp1,&quot;lambda&quot;)</code></pre>
<p><img src="/rmarkdown-libs/figure-html4/unnamed-chunk-10-1.png" width="672" /></p>
<p>This kind of pattern indicates that the sampled values wander all over the parameter space (positive reals), without getting stuck anywhere, suggesting that we have a faithful representation of the posterior distribution.</p>
<div id="extracting-from-the-posterior-and-predictive-distribution" class="section level3">
<h3>Extracting from the posterior and predictive distribution</h3>
<p>The output from <code>pp1</code> summarizes the posterior distribution (and posterior predictive distribution), but if we want to draw pictures of them, we need the actual simulated values. These are obtained by “extracting” from the sampling object:</p>
<pre class="r"><code>ppp1=extract(pp1)
str(ppp1)</code></pre>
<pre><code>## List of 3
##  $ lambda: num [1:4000(1d)] 3.12 3.25 3.49 3.47 2.98 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 1
##   .. ..$ iterations: NULL
##  $ x_new : num [1:4000(1d)] 4 3 4 1 2 2 3 1 2 4 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 1
##   .. ..$ iterations: NULL
##  $ lp__  : num [1:4000(1d)] 28.5 28.6 28.1 28.2 28 ...
##   ..- attr(*, &quot;dimnames&quot;)=List of 1
##   .. ..$ iterations: NULL</code></pre>
<p>Thus the simulated posterior distribution of <code>lambda</code> and the posterior predictive distribution of <code>x</code> are elements of the list <code>ppp1</code>. There are 4000 elements of each, <span class="math inline">\(4 \times 1000\)</span> non-warm-up sampled values. The way the predictive distribution is obtained is, for each value of <code>lambda</code>, a randomly-sampled Poisson with that mean is drawn. Several realizations of the posterior predictive distribution could be obtained by making <code>x_new</code> into an array.</p>
<p>What does the posterior distribution of <code>lambda</code> look like?</p>
<pre class="r"><code>tibble(lambda=ppp1$lambda) %&gt;% 
  ggplot(aes(x=lambda))+
  geom_histogram(bins=10)</code></pre>
<p><img src="/rmarkdown-libs/figure-html4/unnamed-chunk-12-1.png" width="672" /></p>
<p>This looks very much bell-shaped, as was suggested by the largish sample size and population mean that was evidently not too close to zero. (The theory says that the sample total will have a Poisson distribution with a mean of <span class="math inline">\(50 \times 3=150\)</span>, which should be acceptably normal in shape.)</p>
<p>Let’s go back and look at the distribution of the data, for which we should use a bar plot rather than a histogram since the data are discrete:</p>
<pre class="r"><code>tibble(x=x) %&gt;% 
  ggplot(aes(x=x))+
  geom_bar()</code></pre>
<p><img src="/rmarkdown-libs/figure-html4/unnamed-chunk-13-1.png" width="672" /></p>
<p>This is skewed to the right, but it should resemble the posterior predictive distribution, which we can also plot:</p>
<pre class="r"><code>tibble(x_new=ppp1$x_new) %&gt;% 
  ggplot(aes(x=x_new))+
  geom_bar()</code></pre>
<p><img src="/rmarkdown-libs/figure-html4/unnamed-chunk-14-1.png" width="672" /></p>
<p>The actual data has an observed value of 11, which is unlikely but still just possible according to the posterior predictive distribution, and the shapes of the distributions are otherwise similar, so I would call this acceptably good.</p>
</div>
</div>
<div id="real-data-english-soccer-scores" class="section level2">
<h2>Real data: English soccer scores</h2>
<p>I mentioned earlier that a smart way to develop a model in Stan is to fit to data of a known distribution before checking that it behaves itself before letting it loose on real data. The model seems to work, so let’s take some real data now. I’ve chosen soccer scores from England for the 2017-2018 season up to the current date (Febrary 28, 2018) which are available from <a href="http://www.football-data.co.uk/downloadm.php">here</a>, and I saved on my local machine:</p>
<pre class="r"><code>england=read_excel(&quot;/home/ken/all-euro-data-2017-2018.xlsx&quot;,sheet = &quot;E0&quot;)
england</code></pre>
<pre><code>## # A tibble: 279 x 65
##    Div   Date                HomeTeam    AwayTeam   FTHG  FTAG FTR    HTHG
##    &lt;chr&gt; &lt;dttm&gt;              &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt;
##  1 E0    2017-08-11 00:00:00 Arsenal     Leicester  4.00  3.00 H      2.00
##  2 E0    2017-08-12 00:00:00 Brighton    Man City   0     2.00 A      0   
##  3 E0    2017-08-12 00:00:00 Chelsea     Burnley    2.00  3.00 A      0   
##  4 E0    2017-08-12 00:00:00 Crystal Pa… Huddersf…  0     3.00 A      0   
##  5 E0    2017-08-12 00:00:00 Everton     Stoke      1.00  0    H      1.00
##  6 E0    2017-08-12 00:00:00 Southampton Swansea    0     0    D      0   
##  7 E0    2017-08-12 00:00:00 Watford     Liverpool  3.00  3.00 D      2.00
##  8 E0    2017-08-12 00:00:00 West Brom   Bournemo…  1.00  0    H      1.00
##  9 E0    2017-08-13 00:00:00 Man United  West Ham   4.00  0    H      1.00
## 10 E0    2017-08-13 00:00:00 Newcastle   Tottenham  0     2.00 A      0   
## # ... with 269 more rows, and 57 more variables: HTAG &lt;dbl&gt;, HTR &lt;chr&gt;,
## #   Referee &lt;chr&gt;, HS &lt;dbl&gt;, AS &lt;dbl&gt;, HST &lt;dbl&gt;, AST &lt;dbl&gt;, HF &lt;dbl&gt;,
## #   AF &lt;dbl&gt;, HC &lt;dbl&gt;, AC &lt;dbl&gt;, HY &lt;dbl&gt;, AY &lt;dbl&gt;, HR &lt;dbl&gt;, AR &lt;dbl&gt;,
## #   B365H &lt;dbl&gt;, B365D &lt;dbl&gt;, B365A &lt;dbl&gt;, BWH &lt;dbl&gt;, BWD &lt;dbl&gt;,
## #   BWA &lt;dbl&gt;, IWH &lt;dbl&gt;, IWD &lt;dbl&gt;, IWA &lt;dbl&gt;, LBH &lt;dbl&gt;, LBD &lt;dbl&gt;,
## #   LBA &lt;dbl&gt;, PSH &lt;dbl&gt;, PSD &lt;dbl&gt;, PSA &lt;dbl&gt;, WHH &lt;dbl&gt;, WHD &lt;dbl&gt;,
## #   WHA &lt;dbl&gt;, VCH &lt;dbl&gt;, VCD &lt;dbl&gt;, VCA &lt;dbl&gt;, Bb1X2 &lt;dbl&gt;, BbMxH &lt;dbl&gt;,
## #   BbAvH &lt;dbl&gt;, BbMxD &lt;dbl&gt;, BbAvD &lt;dbl&gt;, BbMxA &lt;dbl&gt;, BbAvA &lt;dbl&gt;,
## #   BbOU &lt;dbl&gt;, `BbMx&gt;2.5` &lt;dbl&gt;, `BbAv&gt;2.5` &lt;dbl&gt;, `BbMx&lt;2.5` &lt;dbl&gt;,
## #   `BbAv&lt;2.5` &lt;dbl&gt;, BbAH &lt;dbl&gt;, BbAHh &lt;dbl&gt;, BbMxAHH &lt;dbl&gt;,
## #   BbAvAHH &lt;dbl&gt;, BbMxAHA &lt;dbl&gt;, BbAvAHA &lt;dbl&gt;, PSCH &lt;dbl&gt;, PSCD &lt;dbl&gt;,
## #   PSCA &lt;dbl&gt;</code></pre>
<p><label for="tufte-mn-" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-" class="margin-toggle"><span class="marginnote">This is where I need to learn about the here package.</span></p>
<p>Let’s model the home team’s goals as Poisson with a mean we’ll estimate, which are in the column <code>FTHG</code>. We’ve done most of the work already; we just have to set up the data and sample from the model. My considerable experience with watching soccer says that the home team scores about 2 goals on average:</p>
<pre class="r"><code>my_data_2=list(n=279,x=england$FTHG,prior_mean=2)
pp2=sampling(x_compiled,data=my_data_2)</code></pre>
<pre><code>## 
## SAMPLING FOR MODEL &#39;97d657b45f98f78c52f7aedd24c13437&#39; NOW (CHAIN 1).
## 
## Gradient evaluation took 1.3e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  200 / 2000 [ 10%]  (Warmup)
## Iteration:  400 / 2000 [ 20%]  (Warmup)
## Iteration:  600 / 2000 [ 30%]  (Warmup)
## Iteration:  800 / 2000 [ 40%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.112087 seconds (Warm-up)
##                0.128665 seconds (Sampling)
##                0.240752 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;97d657b45f98f78c52f7aedd24c13437&#39; NOW (CHAIN 2).
## 
## Gradient evaluation took 1.7e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  200 / 2000 [ 10%]  (Warmup)
## Iteration:  400 / 2000 [ 20%]  (Warmup)
## Iteration:  600 / 2000 [ 30%]  (Warmup)
## Iteration:  800 / 2000 [ 40%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.058347 seconds (Warm-up)
##                0.035213 seconds (Sampling)
##                0.09356 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;97d657b45f98f78c52f7aedd24c13437&#39; NOW (CHAIN 3).
## 
## Gradient evaluation took 1.5e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.15 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  200 / 2000 [ 10%]  (Warmup)
## Iteration:  400 / 2000 [ 20%]  (Warmup)
## Iteration:  600 / 2000 [ 30%]  (Warmup)
## Iteration:  800 / 2000 [ 40%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.041821 seconds (Warm-up)
##                0.040131 seconds (Sampling)
##                0.081952 seconds (Total)
## 
## 
## SAMPLING FOR MODEL &#39;97d657b45f98f78c52f7aedd24c13437&#39; NOW (CHAIN 4).
## 
## Gradient evaluation took 1.2e-05 seconds
## 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.
## Adjust your expectations accordingly!
## 
## 
## Iteration:    1 / 2000 [  0%]  (Warmup)
## Iteration:  200 / 2000 [ 10%]  (Warmup)
## Iteration:  400 / 2000 [ 20%]  (Warmup)
## Iteration:  600 / 2000 [ 30%]  (Warmup)
## Iteration:  800 / 2000 [ 40%]  (Warmup)
## Iteration: 1000 / 2000 [ 50%]  (Warmup)
## Iteration: 1001 / 2000 [ 50%]  (Sampling)
## Iteration: 1200 / 2000 [ 60%]  (Sampling)
## Iteration: 1400 / 2000 [ 70%]  (Sampling)
## Iteration: 1600 / 2000 [ 80%]  (Sampling)
## Iteration: 1800 / 2000 [ 90%]  (Sampling)
## Iteration: 2000 / 2000 [100%]  (Sampling)
## 
##  Elapsed Time: 0.041051 seconds (Warm-up)
##                0.04119 seconds (Sampling)
##                0.082241 seconds (Total)</code></pre>
<pre class="r"><code>pp2</code></pre>
<pre><code>## Inference for Stan model: 97d657b45f98f78c52f7aedd24c13437.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##           mean se_mean   sd    2.5%     25%     50%     75%  97.5% n_eff
## lambda    1.55    0.00 0.07    1.40    1.50    1.55    1.59    1.7  1459
## x_new     1.54    0.02 1.25    0.00    1.00    1.00    2.00    4.0  3719
## lp__   -244.40    0.02 0.71 -246.46 -244.55 -244.11 -243.95 -243.9  2062
##        Rhat
## lambda    1
## x_new     1
## lp__      1
## 
## Samples were drawn using NUTS(diag_e) at Wed Mar 21 12:19:33 2018.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>Two goals was a bit of an overestimate: the posterior mean is 1.55, with a 95% posterior interval going from 1.41 to 1.70.</p>
<p>Again, we should check that the data distribution and the posterior predictive distribution match up reasonably well:</p>
<pre class="r"><code>tibble(x=my_data_2$x) %&gt;% 
  ggplot(aes(x=x))+
  geom_bar()</code></pre>
<p><img src="/rmarkdown-libs/figure-html4/unnamed-chunk-17-1.png" width="672" /></p>
<p>and</p>
<pre class="r"><code>ppp2=extract(pp2)
tibble(x=ppp2$x_new) %&gt;% 
  ggplot(aes(x=x))+
  geom_bar()</code></pre>
<p><img src="/rmarkdown-libs/figure-html4/unnamed-chunk-18-1.png" width="672" /></p>
<p>This is not quite so good: there are in actuality more teams scoring 4 goals (and, to some extent, 5) and fewer scoring 3 than the posterior predictive distribution suggests. One explanation for this is that the home scores are not all Poisson with the same mean, because the likely number of goals depends on the strengths of the home and away teams; the true distribution is probably not Poisson at all, but might be some kind of mixture of Poissons of varying means - that is to say, we would expect overdispersion relative to a Poisson model.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>I was planning to add something to this post about Bayesian Poisson regression, but I’ll save that for another post. Most of the ideas are similar, but there is some elaboration needed.</p>
<p>A better way to handle the soccer data is as Poisson, but with a mean that depends on the attacking and defensive capabilities of the teams playing. That too will have to wait for another time.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p><a href="http://m-clark.github.io/workshops/bayesian/03_stan.html" class="uri">http://m-clark.github.io/workshops/bayesian/03_stan.html</a> reminded me of a lot of what I’d forgotten.</p>
<p><a href="http://mc-stan.org/events/stancon2017-notebooks/stancon2017-kharratzadeh-epl.pdf" class="uri">http://mc-stan.org/events/stancon2017-notebooks/stancon2017-kharratzadeh-epl.pdf</a> is a cool piece of modelling that I want to borrow from in the future.</p>
<p><a href="https://github.com/stan-dev/stan/releases/download/v2.17.0/stan-reference-2.17.0.pdf" class="uri">https://github.com/stan-dev/stan/releases/download/v2.17.0/stan-reference-2.17.0.pdf</a> is the bible of Stan. If you want it, it’s in here somewhere.</p>
</div>
