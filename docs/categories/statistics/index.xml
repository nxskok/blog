<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on Ken&#39;s Blog</title>
    <link>/categories/statistics/</link>
    <description>Recent content in Statistics on Ken&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 25 Mar 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Today on Twitter I learned...</title>
      <link>/2018/03/25/today-on-twitter-i-learned/</link>
      <pubDate>Sun, 25 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/25/today-on-twitter-i-learned/</guid>
      <description>Introduction Today on Twitter I learned (or was reminded about) two #rstats things:
from @pkqstr about separate_rows from tidyr, that does something like separate followed by gather, but better. from @ma_salmon about haven for reading in data files from other software, and I thought about rio that does more or less the same thing, but more generally.  I didn’t come up with an answer to Maëlle’s question of why haven worked less well for me than rio a long time ago.</description>
    </item>
    
    <item>
      <title>Ward&#39;s method and dissimilarities</title>
      <link>/2018/03/22/ward-s-method-and-dissimilarities/</link>
      <pubDate>Thu, 22 Mar 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/03/22/ward-s-method-and-dissimilarities/</guid>
      <description>Introduction I don’t know yet where this post is going. Think of it, for now, as a ramble through cluster analysis. I may eventually figure out what to do with it, but I don’t want to delete what I have written just yet.
Hierarchical clustering is a way of forming groups or “clusters” of like individuals. The various forms of hierarchical clustering work from distances or dissimilarilities between individuals. The process is to start from each individual in a cluster by itself and then to join the closest pair of clusters one by one until all individuals are in a single cluster.</description>
    </item>
    
    <item>
      <title>Working my way back to you, a re-investigation of rstan</title>
      <link>/2018/02/28/working-my-way-back-to-you-a-re-investigation-of-rstan/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/28/working-my-way-back-to-you-a-re-investigation-of-rstan/</guid>
      <description>Introduction I learned Stan a while back, when I was fitting some Bayesian models. I wanted to fix up one of them, and I realized that I had forgotten most of what I knew about Stan, so I had to go back and learn it again.
A Bayesian model has two parts: a prior distribution, which summarizes your belief about the parameters you are trying to estimate before you look at any data, and a model that asserts the data-generating mechanism conditional on the parameter value(s).</description>
    </item>
    
    <item>
      <title>A brief foray into list-columns</title>
      <link>/2017/07/25/a-brief-foray-into-list-columns/</link>
      <pubDate>Tue, 25 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/25/a-brief-foray-into-list-columns/</guid>
      <description>Introduction Let’s suppose we are trying to find the median of a bunch of binomial distributions. ⊕This is a simplified version of an actual problem I had, but that one fails for some unconnected (and thus far unknown) reason, so I don’t want to show you that.
To be specific, let’s suppose we have these values of \(n\):
n=c(10,15,20) and these values of \(p\):
p=c(0.25,0.3,0.42) We’ll need the tidyverse, as usual:</description>
    </item>
    
    <item>
      <title>Summarizing several models using broom and purrr</title>
      <link>/2017/07/20/summarizing-several-models-using-broom-and-purrr/</link>
      <pubDate>Thu, 20 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/07/20/summarizing-several-models-using-broom-and-purrr/</guid>
      <description>Introduction broom is supposed to be a powerful way to summarize several models at once, and so it is. The trouble is, the examples show how to fit the same model to different subsets of a data set. I had something different in mind: I had one data set, and three different models on that same data. Could I do something similar there? It wasn’t clear to me how.
 Illustrative data The data here came from Albright’s book.</description>
    </item>
    
    <item>
      <title>Histograms and bins</title>
      <link>/2017/06/08/histograms-and-bins/</link>
      <pubDate>Thu, 08 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/08/histograms-and-bins/</guid>
      <description>Most software, when you ask it to draw you a histogram, will choose a number of intervals (“bins”) for you. Base R is one of those. To illustrate, let’s read in some data:
library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ── ## ✔ ggplot2 2.2.1.9000 ✔ purrr 0.2.4 ## ✔ tibble 1.4.2 ✔ dplyr 0.7.4 ## ✔ tidyr 0.8.0 ✔ stringr 1.3.0 ## ✔ readr 1.1.1 ✔ forcats 0.3.0 ## ── Conflicts ─────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() myurl=&amp;quot;http://www.</description>
    </item>
    
    <item>
      <title>Carter and Guthrie</title>
      <link>/2017/06/01/carter-and-guthrie/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/06/01/carter-and-guthrie/</guid>
      <description>Introduction Carter and Guthrie, in 2004, proposed a method of modelling cricket matches. Their aim was to provide an alternative method of deciding interrupted matches, in the manner of Duckworth and Lewis. What was interesting to me is that they estimate a probability of winning (which is then held fixed over interruptions), and it seemed to me that one could estimate and update the probability of winning as the game progresses, which would be a useful adjunct for spectators.</description>
    </item>
    
    <item>
      <title>Add-in</title>
      <link>/2017/05/22/add-in/</link>
      <pubDate>Mon, 22 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/22/add-in/</guid>
      <description>I just discovered a couple of things:
an R Studio add-in called CRANsearcher that, when you run it, prompts you for search terms and searches the whole of CRAN for anything that matches those search terms. (Thanks to @juliasilge on Twitter for this.) To install:  devtools::install_github(&amp;quot;RhoInc/CRANsearcher&amp;quot;) This inspired me to see what else was on my AddIns menu in R Studio. I also found options for creating a new blog post and “serving the site” so I can preview it.</description>
    </item>
    
    <item>
      <title>Welch analysis of variance</title>
      <link>/2017/05/19/welch-analysis-of-variance/</link>
      <pubDate>Fri, 19 May 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/05/19/welch-analysis-of-variance/</guid>
      <description>Introduction The standard analysis of variance based on the \(F\)-test has two major assumptions:
Normally distributed data Equal variance within each group.  The analysis can handle a certain amount of non-normality, but the equal-variance assumption is important because it is required for the idea of “an” error variance to make sense (that is what the error mean square is estimating).
Is it possible to make an ANOVA that can allow for the groups to have different variances?</description>
    </item>
    
  </channel>
</rss>