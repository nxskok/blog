
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">

<head>
<title>R, it&#39;s OK I guess - Estimating sigma using quantiles</title>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="">
<meta name="keywords" content="">
<meta name="author" content="R, it&#39;s OK I guess">
<meta name="generator" content="Hugo 0.73.0" />

  





<link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.6.0/pure-min.css">


    <link rel="stylesheet" href="http://yui.yahooapis.com/pure/0.6.0/grids-responsive-min.css">








<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css">


<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>


<link rel="stylesheet" href="../../../../css/tuftesque.css">
<script src="../../../../js/lightbox.js"></script>
<style>
body {
  
    background-color: #fffff8;
  
}
</style>



<script>

</script>


</head>

<body>
<div id="layout" class="pure-g">
<article class="pure-u-1">
<header class="brand">
  <h1>
    <a href="../../../../">
      <span id = "blog_logo">
         <img src="http://www.utsc.utoronto.ca/~butler/156-front.jpg" alt="Blog Logo" style="height: 40px; width:40px">
      </span>
      
    </a>
  </h1>
</header>

<section>
  <h1 class="content-title">
  
  <a href="../../../../2018/12/13/estimating-sigma-using-quantiles/">Estimating sigma using quantiles</a>
  
  </h1>
  
  
  
  <span class="content-meta">
    
  
  
  
  <i class="fa fa-user">&nbsp;</i><span class="author">
    &nbsp;Ken</span> <br>
    
  
  
  
  <i class="fa fa-calendar"></i>
    &nbsp;Dec 13, 2018
  
  
  
  &nbsp;<i class="fa fa-clock-o"></i>
    &nbsp;13 min read
  
  
  
  <br>
    <i class="fa fa-tags"> </i>
    
    <a  href="../../../../categories/r">R</a>
    
    <a  href="../../../../categories/statistics">statistics</a>
    
    
    </span>
    
    
    </section>


<section>


<div id="packages" class="section level2">
<h2>Packages</h2>
<p>There will be some R later, with some random number generation. I set the random number seed for reproducibility:</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## ── Attaching packages ─────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──</code></pre>
<pre><code>## ✔ ggplot2 3.1.0     ✔ purrr   0.3.0
## ✔ tibble  2.0.1     ✔ dplyr   0.7.8
## ✔ tidyr   0.8.2     ✔ stringr 1.4.0
## ✔ readr   1.3.1     ✔ forcats 0.3.0</code></pre>
<pre><code>## ── Conflicts ────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>set.seed(457299)</code></pre>
</div>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>So, the other day, I was making normal quantile plots using SAS. As you do. I was using Simon Newcomb’s famous data on the speed of light, and I used this code:</p>
<pre><code>proc univariate noprint;
  qqplot Time / normal(mu=est sigma=est);</code></pre>
<p>and got this output:</p>
<p><img src="../../../../qqplot1.png" /></p>
<p>The idea of a normal quantile plot is that if your data really comes from a normal distribution, the points will follow the line (at least approximately), and the way in which they don’t follow the line tells you how your data values are not normally distributed. I looked at this one and thought “well, these are not too bad, apart from that outlier at the bottom left”, and the message seems to be that one of those values is much lower than the others. Thus, if you were using these data to estimate the true speed of light, it would be dangerous to use the sample mean, because it will be pulled downwards by that low value.</p>
<p>The way SAS adds a reference line to the normal quantile plot is to ask you to specify <code>mu</code> and <code>sigma</code> for the normal distribution. If you don’t want to specify them, SAS is happy to estimate them, which it does the standard way using the sample mean and standard deviation. Or you can specify numeric values for them.</p>
<p>But, I thought, when you have outliers, or skewness, which you often will when you are using a normal quantile plot, the sample mean and standard deviation are the last things you should be using. But, SAS asks you to supply numbers for <code>mu</code> and <code>sigma</code>. What numbers are you going to use?</p>
</div>
<div id="estimating-mu-and-sigma-using-quantiles" class="section level2">
<h2>Estimating <code>mu</code> and <code>sigma</code> using quantiles</h2>
<p>A normal distribution is symmetric, so <code>mu</code> is equal to both its mean and its median. So, estimating <code>mu</code> is easy: if you are worried about outliers, use the sample median instead of the sample mean.</p>
<p>But how to estimate <code>sigma</code>? It is a measure of spread, so something to think about instead of the standard deviation would be a measure of spread like the interquartile range. This, like the median, won’t be affected by outliers.</p>
<p>In a standard normal distribution, the quartiles are about <span class="math inline">\(\pm 0.675\)</span>:</p>
<pre class="r"><code>q=c(0.25,0.75)
qnorm(q)</code></pre>
<pre><code>## [1] -0.6744898  0.6744898</code></pre>
<p>which means that the interquartile range is about <span class="math inline">\(0.675-(-0.675)=1.35\)</span>. That is, the IQR is 1.35 times as big as the SD. A normal distribution with SD <span class="math inline">\(\sigma\)</span> is a standard normal made wider by a factor <span class="math inline">\(\sigma\)</span> (and translated sideways by the mean), so the IQR of a general normal distribution is <span class="math inline">\(1.35\sigma\)</span>. This means that we can estimate <span class="math inline">\(\sigma\)</span> by taking the sample interquartile range and dividing it by 1.35.</p>
<p>This may not be very efficient (in the sense that, if the data really are normal, you can estimate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> more accurately using the sample mean and SD), but our main aim here is to protect ourselves against trouble, and so this ought to be a reasonable idea. How does it play out here? Here is some of the Newcomb data:</p>
<pre class="r"><code>Newcomb %&gt;% slice(1:10)</code></pre>
<pre><code>##          Time Series
## 1  2.4828e-05      1
## 2  2.4826e-05      1
## 3  2.4833e-05      1
## 4  2.4824e-05      1
## 5  2.4834e-05      1
## 6  2.4756e-05      1
## 7  2.4827e-05      1
## 8  2.4816e-05      1
## 9  2.4840e-05      1
## 10 2.4798e-05      1</code></pre>
<p>The median and IQR are:</p>
<pre class="r"><code>(Newcomb %&gt;% summarize(med=median(Time), iqr=IQR(Time)) -&gt; d)</code></pre>
<pre><code>##          med      iqr
## 1 2.4827e-05 6.75e-09</code></pre>
<p>and thus our estimate of <span class="math inline">\(\sigma\)</span> is</p>
<pre class="r"><code>d$iqr/1.35</code></pre>
<pre><code>## [1] 5e-09</code></pre>
<p>Putting those values in for <code>mu</code> and <code>sigma</code> gives this normal quantile plot:</p>
<p><img src="../../../../qqplot2.png" /></p>
<p>The story is now quite different: there are actually <em>two</em> outliers at the bottom, and the rest of the observations are very close to what you’d expect in a normal distribution. This suggests that the bottom two observations are actually errors. Using the IQR to estimate <code>sigma</code> with seems to work very well for this purpose. This is the kind of thing R does with its normal quantile plot:</p>
<pre class="r"><code>ggplot(Newcomb, aes(sample=Time))+stat_qq()+stat_qq_line()</code></pre>
<p><img src="../../../../post/2018-12-13-estimating-sigma-using-quantiles_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The help file says that the line goes through the observed and theoretical quartiles.</p>
</div>
<div id="using-other-quantiles-to-estimate-sigma" class="section level2">
<h2>Using other quantiles to estimate <span class="math inline">\(\sigma\)</span></h2>
<p>But, I only used the quartiles because the IQR happened to come to mind as a measure of spread. The normal distribution is symmetric, so any pair of quantiles symmetrically placed about the mean (median) could be used, such as the 40th and 60th percentiles, or the 10th and 90th percentiles. If you choose percentiles further apart, you’ll need to divide by something bigger to get a sensible estimate of <span class="math inline">\(\sigma\)</span>. How big? Well,</p>
<pre class="r"><code>qnorm(0.90)-qnorm(0.10)</code></pre>
<pre><code>## [1] 2.563103</code></pre>
<p>that big, if we use the 10th and 90th percentiles. <label for="tufte-mn-" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-" class="margin-toggle"><span class="marginnote">The discussion earlier also applies here: this is for a standard normal, but if the SD is not 1, the difference between 10th and 90th percentiles will still be 2.56 times whatever the SD is.</span></p>
<p>It seems like we should have a function that takes a quantile and returns that difference that we will need to divide by:</p>
<pre class="r"><code>divisor=function(p) {
  abs(qnorm(p)-qnorm(1-p))
}</code></pre>
<p>and to test it on values where we know the answer already:</p>
<pre class="r"><code>divisor(c(0.10,0.25,0.75,0.90))</code></pre>
<pre><code>## [1] 2.563103 1.348980 1.348980 2.563103</code></pre>
<p>I used the <code>abs</code> in the function to allow for input of either the high end or the low end.</p>
</div>
<div id="finding-the-best-quantiles-to-estimate-sigma-with" class="section level2">
<h2>Finding the best quantiles to estimate <span class="math inline">\(\sigma\)</span> with</h2>
<p>I’m thinking that if you use a pair of quantiles too close to the centre of the distribution, you won’t get too much sense of the spread, and if you use a pair of quantiles too far out in the tails, you’ll get done in by the extremeness of the extreme values you happen to observe (which, in real data, might be outliers). So there ought to be a happy medium: a pair of quantiles not too far out that will estimate <span class="math inline">\(\sigma\)</span> more accurately than any other pair.</p>
<p>We can do a simulation to find out whether this idea is right, and if it is, where the happy medium lies. The similarity in shape of the normal distribution for any value of <span class="math inline">\(\sigma\)</span> means that we can simulate from the standard normal and the results we get will apply for any <span class="math inline">\(\sigma\)</span>.</p>
<p>Let’s make a list of the quantiles we’re going to use in the simulation (the low end):</p>
<pre class="r"><code>qq=c(0.01,0.025,0.05,0.075,0.1,0.15,0.20,0.25,0.4)</code></pre>
<p>and generate some random normal data:</p>
<pre class="r"><code>(z=rnorm(50))</code></pre>
<pre><code>##  [1]  1.621867352 -0.746347365 -0.268930797 -0.699535090  0.213237930
##  [6]  0.708968535 -1.078329045  0.791310415  0.004046959  1.095879569
## [11] -1.655475142 -1.206874304  1.268749118  0.838393233 -0.746106341
## [16]  0.052753612  1.514875388 -0.112308710  0.266535207 -1.720378300
## [21]  1.160778666  0.471876517  0.944608052 -0.807257675  0.279188834
## [26]  0.686444356  0.607641604  0.071344093 -0.154485997 -1.177612022
## [31]  0.012286032 -0.644232045  1.402745042 -0.865977059  1.324754616
## [36]  1.468643130  1.196362551  1.434646249 -0.579221119  0.378456915
## [41] -0.166149025 -0.644332211 -2.158475531 -1.158177890  0.519147525
## [46] -0.996153379 -0.112548668  0.203054949 -2.051009895  0.083031555</code></pre>
<p>Let’s make a table of divisors (that we calculate once and re-use):</p>
<pre class="r"><code>(tibble(lo=qq, hi=1-qq) %&gt;% 
  mutate(div=divisor(lo)) -&gt; divs)</code></pre>
<pre><code>## # A tibble: 9 x 3
##      lo    hi   div
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1 0.01  0.99  4.65 
## 2 0.025 0.975 3.92 
## 3 0.05  0.95  3.29 
## 4 0.075 0.925 2.88 
## 5 0.1   0.9   2.56 
## 6 0.15  0.85  2.07 
## 7 0.2   0.8   1.68 
## 8 0.25  0.75  1.35 
## 9 0.4   0.6   0.507</code></pre>
<p>and now we have to obtain all those quantiles for our random data, work out the differences, and divide by the right divisor:</p>
<pre class="r"><code>divs %&gt;% mutate(z_lo=quantile(z,lo), z_hi=quantile(z,hi)) %&gt;% 
  mutate(sigma_hat=(z_hi-z_lo)/div)</code></pre>
<pre><code>## # A tibble: 9 x 6
##      lo    hi   div   z_lo  z_hi sigma_hat
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;
## 1 0.01  0.99  4.65  -2.11  1.57      0.790
## 2 0.025 0.975 3.92  -1.98  1.50      0.888
## 3 0.05  0.95  3.29  -1.69  1.45      0.956
## 4 0.075 0.925 2.88  -1.35  1.41      0.961
## 5 0.1   0.9   2.56  -1.18  1.33      0.980
## 6 0.15  0.85  2.07  -1.05  1.18      1.08 
## 7 0.2   0.8   1.68  -0.819 0.975     1.07 
## 8 0.25  0.75  1.35  -0.734 0.771     1.12 
## 9 0.4   0.6   0.507 -0.129 0.272     0.791</code></pre>
<p>In this case, the estimate of <span class="math inline">\(\sigma\)</span> closest to the true value of 1 is 0.98, for the 10th and 90th percentiles.</p>
<p>We’re going to do this lots of times in a moment, so let’s make this into a function. It will have two inputs: the sample of random normals, and the table of quantiles and divisors. I’ll return only the quantiles and estimates of sigma:</p>
<pre class="r"><code>estimates=function(z,divs) {
  divs %&gt;% mutate(z_lo=quantile(z,lo), z_hi=quantile(z,hi)) %&gt;% 
    mutate(sigma_hat=(z_hi-z_lo)/div) %&gt;% 
    select(q=lo,sigma_hat)
}</code></pre>
<p>To generate many random samples, I’ll use <code>rerun</code>, which is the <code>tidyverse</code> version of the base R <code>replicate</code>. Let’s start from small beginnings:</p>
<pre class="r"><code>rerun(5,rnorm(4))</code></pre>
<pre><code>## [[1]]
## [1]  0.5832606 -0.8601665 -0.3270047 -0.2753042
## 
## [[2]]
## [1]  0.007565819 -0.624116696 -0.062118656  0.233042042
## 
## [[3]]
## [1]  0.05757444 -2.42125707 -0.29280899  0.04249248
## 
## [[4]]
## [1]  0.75268978  0.02598616 -0.25275065 -1.66833677
## 
## [[5]]
## [1]  2.6685238  0.5419123 -0.5986237  1.2115979</code></pre>
<p>This is 5 random samples of size 4, which come out in a list. I recently learned about <code>enframe</code>, which turns vectors and lists into two-column data frames:</p>
<pre class="r"><code>rerun(5,rnorm(4)) %&gt;% 
  enframe()</code></pre>
<pre><code>## # A tibble: 5 x 2
##    name value    
##   &lt;int&gt; &lt;list&gt;   
## 1     1 &lt;dbl [4]&gt;
## 2     2 &lt;dbl [4]&gt;
## 3     3 &lt;dbl [4]&gt;
## 4     4 &lt;dbl [4]&gt;
## 5     5 &lt;dbl [4]&gt;</code></pre>
<p>The first column numbers the random samples, and the second contains the samples themselves, in a list-column. Now that we know how it works, let’s generate bigger samples, and give the columns better names:</p>
<pre class="r"><code>rerun(5,rnorm(50)) %&gt;% 
  enframe(name=&quot;sample_number&quot;, value=&quot;sample&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 2
##   sample_number sample    
##           &lt;int&gt; &lt;list&gt;    
## 1             1 &lt;dbl [50]&gt;
## 2             2 &lt;dbl [50]&gt;
## 3             3 &lt;dbl [50]&gt;
## 4             4 &lt;dbl [50]&gt;
## 5             5 &lt;dbl [50]&gt;</code></pre>
<p>Next, for each of those samples, we want to generate the data frame of estimates of <span class="math inline">\(\sigma\)</span>:</p>
<pre class="r"><code>rerun(5,rnorm(50)) %&gt;% 
  enframe(name=&quot;sample_number&quot;, value=&quot;sample&quot;) %&gt;% 
  mutate(ests=map(sample, ~estimates(.,divs)))</code></pre>
<pre><code>## # A tibble: 5 x 3
##   sample_number sample     ests            
##           &lt;int&gt; &lt;list&gt;     &lt;list&gt;          
## 1             1 &lt;dbl [50]&gt; &lt;tibble [9 × 2]&gt;
## 2             2 &lt;dbl [50]&gt; &lt;tibble [9 × 2]&gt;
## 3             3 &lt;dbl [50]&gt; &lt;tibble [9 × 2]&gt;
## 4             4 &lt;dbl [50]&gt; &lt;tibble [9 × 2]&gt;
## 5             5 &lt;dbl [50]&gt; &lt;tibble [9 × 2]&gt;</code></pre>
<p>and then pull out the estimates so that we can summarize them:</p>
<pre class="r"><code>rerun(5,rnorm(50)) %&gt;% 
  enframe(name=&quot;sample_number&quot;, value=&quot;sample&quot;) %&gt;% 
  mutate(ests=map(sample, ~estimates(.,divs))) %&gt;% 
  unnest(ests)</code></pre>
<pre><code>## # A tibble: 45 x 3
##    sample_number     q sigma_hat
##            &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;
##  1             1 0.01      0.984
##  2             1 0.025     0.978
##  3             1 0.05      1.01 
##  4             1 0.075     1.10 
##  5             1 0.1       1.10 
##  6             1 0.15      1.08 
##  7             1 0.2       1.04 
##  8             1 0.25      1.15 
##  9             1 0.4       1.26 
## 10             2 0.01      0.976
## # … with 35 more rows</code></pre>
<p>Compute the squared error of each estimate, and find the mean squared error for each quantile:</p>
<pre class="r"><code>rerun(5,rnorm(50)) %&gt;% 
  enframe(name=&quot;sample_number&quot;, value=&quot;sample&quot;) %&gt;% 
  mutate(ests=map(sample, ~estimates(.,divs))) %&gt;% 
  unnest(ests) %&gt;% 
  mutate(sq_error=(sigma_hat-1)^2) %&gt;% 
  group_by(q) %&gt;% 
  summarize(mse=mean(sq_error))</code></pre>
<pre><code>## # A tibble: 9 x 2
##       q    mse
##   &lt;dbl&gt;  &lt;dbl&gt;
## 1 0.01  0.0545
## 2 0.025 0.0520
## 3 0.05  0.0449
## 4 0.075 0.0370
## 5 0.1   0.0257
## 6 0.15  0.0160
## 7 0.2   0.0140
## 8 0.25  0.0200
## 9 0.4   0.0433</code></pre>
<p>This was rather a small-scale simulation, but the best here is the 20th and 80th percentiles. Before we do a bigger simulation, let’s put the above into a function with the sample size and number of simulations as inputs:</p>
<pre class="r"><code>simulate=function(n,n_sim) {
  rerun(n_sim,rnorm(n)) %&gt;% 
    enframe(name=&quot;sample_number&quot;, value=&quot;sample&quot;) %&gt;% 
    mutate(ests=map(sample, ~estimates(.,divs))) %&gt;% 
    unnest(ests) %&gt;% 
    mutate(sq_error=(sigma_hat-1)^2) %&gt;% 
    group_by(q) %&gt;% 
    summarize(mse=mean(sq_error))
}</code></pre>
<p>and then do it for real:</p>
<pre class="r"><code>(d=simulate(50,10000))</code></pre>
<pre><code>## # A tibble: 9 x 2
##       q    mse
##   &lt;dbl&gt;  &lt;dbl&gt;
## 1 0.01  0.0258
## 2 0.025 0.0193
## 3 0.05  0.0156
## 4 0.075 0.0151
## 5 0.1   0.0161
## 6 0.15  0.0174
## 7 0.2   0.0206
## 8 0.25  0.0258
## 9 0.4   0.0767</code></pre>
<p>This time the 7.5-92.5 percentile pair is best, and the 40-60 pair is clearly worst. Does that depend on sample size? Showing off a little:</p>
<pre class="r"><code>best_q=function(d) {
  d %&gt;% arrange(mse) %&gt;% pluck(&quot;q&quot;,1)
}
best_q(d)</code></pre>
<pre><code>## [1] 0.075</code></pre>
<pre class="r"><code>tibble(n=c(10,20,50,100)) %&gt;% 
  mutate(sims=map(n,~simulate(.,10000))) %&gt;% 
  mutate(q=map_dbl(sims,~best_q(.)))</code></pre>
<pre><code>## # A tibble: 4 x 3
##       n sims                 q
##   &lt;dbl&gt; &lt;list&gt;           &lt;dbl&gt;
## 1    10 &lt;tibble [9 × 2]&gt; 0.075
## 2    20 &lt;tibble [9 × 2]&gt; 0.075
## 3    50 &lt;tibble [9 × 2]&gt; 0.075
## 4   100 &lt;tibble [9 × 2]&gt; 0.075</code></pre>
<p>The 7.5-92.5 percentiles seem to be consistently the best.</p>
</div>
<div id="this-has-been-done-before" class="section level2">
<h2>This has been done before</h2>
<p>… and back in 1949, at that. Benson (1949) wrote about this, and noted (citing a 1920 paper of Karl Pearson) that the estimator of <span class="math inline">\(\sigma\)</span> with the smallest variance used the 7th and 93rd percentiles, for all sample sizes. My simulations are consistent with this.</p>
<p>Benson also estimated <span class="math inline">\(\mu\)</span> using the mean of a pair of quantiles, and said that the variance of the estimator was minimized when you use the mean of the 27th and 73rd percentiles. <label for="tufte-mn-" class="margin-toggle">⊕</label><input type="checkbox" id="tufte-mn-" class="margin-toggle"><span class="marginnote">This is almost the same thing as the midhinge, where hinge is Tukey’s term for the quartile.</span></p>
</div>
<div id="an-exam-question" class="section level2">
<h2>An exam question</h2>
<p>I had shown my students that you can estimate <span class="math inline">\(\sigma\)</span> by taking the IQR and dividing by 1.35, so on my final exam I played with that idea a bit more. I had them use the 68-95-99.7 rule (which they have seen before) to show that the 16th and 84th
percentiles of a normal distribution are at <span class="math inline">\(\mu \pm \sigma\)</span>, and thus that taking the difference between those two percentiles and dividing by 2 (since they are <span class="math inline">\(2\sigma\)</span> apart) is a sensible estimator of <span class="math inline">\(\sigma\)</span>. (Had I included 0.16 as one of my quantiles above, <code>div</code> for it would have been 2.) I then gave them some data for which the 16th percentile was 26, the 84th percentile was 48, and the median was 36, and asked them to estimate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. I thought the “obvious” estimator of <span class="math inline">\(\mu\)</span> was the median, giving an estimate 36, but quite a lot of people noted that <span class="math inline">\(\mu-\sigma=26\)</span> and <span class="math inline">\(\mu+\sigma=48\)</span>, and then solved the two equations for the two variables, getting estimates of <em>37</em> and 11. Full marks, of course. Little did they know, but on about the 70th anniversary of Benson’s work, they were using his idea.</p>
<p>The idea for all of this came, in fact, from one of my students, who casually asked after class, “couldn’t you use different percentiles to estimate <span class="math inline">\(\sigma\)</span> with?”, and we talked about how you could, but you’d have to divide by something other than 1.35.</p>
</div>
<div id="future-work" class="section level2">
<h2>Future work</h2>
<p>What would be relatively easy now (but this blog post is too long) is to compare the mean squared error of the best percentile-based estimator of <span class="math inline">\(\sigma\)</span> with that of the sample SD. If it turns out that the mean squared error of the percentile estimator is not too much bigger, then we could recommend using the percentile estimator because of the additional protection it gives when there are outliers. (This is the same idea as using the Welch-Satterthwaite <span class="math inline">\(t\)</span>-test in favour of the pooled one; W-S is not the best test if the two groups have the same variance, but it is a <em>lot</em> better if the variances are actually different.)</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>Reynold Jong (2018, personal communication, ie. a chat after class).</p>
<p><a href="http://people.reed.edu/~jones/141/Newcomb.html">Newcomb data</a></p>
<p>Benson, F. (1949). A Note on the Estimation of Mean and Standard Deviation from Quantiles. Journal of the Royal Statistical Society. Series B (Methodological), 11(1), 91-100. Retrieved from <a href="http://www.jstor.org.myaccess.library.utoronto.ca/stable/2983699">here</a></p>
<p><a href="https://en.wikipedia.org/wiki/Midhinge">The midhinge</a></p>
</div>
</section>
<section>
  

  



  
  <footer class="page-footer">
		<hr>
		<ul class="page-footer-menu">
		
		</ul>

  

	<div class="copyright">
	<p>
    
      &copy;2020
    .
    All rights reserved.
    
  </p>
</div>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
      }
    });
    </script>
    <script type="text/javascript"
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</footer>

</section>
</article>
</div>
</body>
</html>
